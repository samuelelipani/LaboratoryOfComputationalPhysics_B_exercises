{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3a305b4d08dfd919fc77ef9206769bd32a0bd25ca54dc32c0a5bb8b4ec4e7e89"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Some packages \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "# dropout is supposed to avoid a bit of overfitting becasue it allows the NN not depending on a particular node removing for each iteration one of them\n",
    "from keras.optimizers import SGD \n",
    "import time \n",
    "import math "
   ]
  },
  {
   "source": [
    "## Data (strings) and splitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000\n12\nTAGGCGTCGATG 0\n\ndata: 10000\ntrain: 8000\ntest: 2000\n"
     ]
    }
   ],
   "source": [
    "fname = 'DATA/sequences12.csv'\n",
    "sx, sy = np.loadtxt(fname, delimiter=',',\n",
    "                    usecols=(0,1),\n",
    "                    unpack=True, \n",
    "                    dtype = str)\n",
    "N = len(sy)\n",
    "print(N)\n",
    "# essendo tante labels quanti sono i dati \n",
    "\n",
    "Ls = len(sx[0])\n",
    "print(Ls)\n",
    "# mentre Ls altro non è che la lunghezza di ogni array estratto dal file csv presa singolarmente\n",
    "\n",
    "print(sx[0], sy[0])\n",
    "\n",
    "perc_train = 0.8\n",
    "N_train = int(N* perc_train)\n",
    "N_test = N - N_train \n",
    "print(f'\\ndata: {N}\\ntrain: {N_train}\\ntest: {N_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'A': 0, 'C': 1, 'G': 2, 'T': 3}\n"
     ]
    }
   ],
   "source": [
    "# it is necessary to transform our sequences of letters into numbers, converting every piece of the data into a sequence of bits depending on a dictionary of characters we are going to make \n",
    "Q = ['A','C','G','T']\n",
    "Nc = 4\n",
    "onehc = {Q[i]:i for i in range(Nc)}\n",
    "print(onehc)\n",
    "# the dictionary is created like in a list comprehension \n",
    "# con il one hot encoding stiamo convertendo ogni lettera in una sequanza di bits: \n",
    "# A : 1000, C : 0100, G : 0010, T : 0001\n",
    "# mentre la scelta di attribuire a ciascuna lettera un numero reale potrebbe essere arbitraria e far nascere qualche tipo di preferenza nella classificazione all'interno del neural network, utilizzando sequenze di 4 bits evitiamo questo problema perchè sono tutte e quattro equiprobabili in un certo senso\n"
   ]
  },
  {
   "source": [
    "### Data conversion \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nTAGGCGTCGATG\n[0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y = sy.astype(int) # casting everything int\n",
    "# convertire le labels è semplice perchè sono N interi 0 oppure 1\n",
    "\n",
    "L = Ls*Nc\n",
    "print(L)\n",
    "# perchè ogni lettera è tradotta in quattro bits\n",
    "\n",
    "x = np.zeros((N,L))\n",
    "print(x[0])\n",
    "\n",
    "for n in range(N): \n",
    "# quindi per tutte le righe \n",
    "    for i in range(Ls): \n",
    "# per ogni lettera \n",
    "        x[n][i*4 + onehc[sx[n][i]]] = 1 \n",
    "# 4*i perchè per ogni step di una lettera abbiamo uno spostamento di quattro bits nella sequenza convertita.\n",
    "# Vediamo che nel dizionario il valore che accompagna ciascuna carattere corrisponde alla posizione del numero uno nella sequenza di 4 bits, quindi scegliendo una riga x[n] cambiamo ogni quattro bits il valore alla posizione onehc[sx[n][i]] e lo poniamo uguale a uno. (con onehc[] scegliamo il valore della chiave corrispondente alla lettera che andremo a mettere tra parentesi essendo onehc un dizionario)\n",
    "\n",
    "print(sx[0])\n",
    "print(x[0])"
   ]
  },
  {
   "source": [
    "### Split train / test-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.314125\n0.3165\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = x[:N_train], y[:N_train]\n",
    "x_test, y_test = x[:N_test], y[:N_test]\n",
    "\n",
    "print(y_train.sum() / N_train)\n",
    "print(y_test.sum() / N_test)"
   ]
  },
  {
   "source": [
    "### Definition of model in Keras\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 48)                2352      \n_________________________________________________________________\ndense_1 (Dense)              (None, 24)                1176      \n_________________________________________________________________\ndense_2 (Dense)              (None, 12)                300       \n_________________________________________________________________\ndropout (Dropout)            (None, 12)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 13        \n=================================================================\nTotal params: 3,841\nTrainable params: 3,841\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#I choose narrower dimensions of layers in my model (that's my choice)\n",
    "#also activation is my choice\n",
    "model.add(Dense(L,input_shape=(L,),activation=\"relu\")) #set input layer of shape (1,0)\n",
    "model.add(Dense(L/2,activation=\"relu\"))\n",
    "model.add(Dense(L/4,activation=\"relu\"))\n",
    "model.add(Dropout(0.2)) #drop non active nodes in the last layer(?)\n",
    "\n",
    "#now I change the activation function because I want a prediciton as a probability of being 1\n",
    "model.add(Dense(1,activation=\"sigmoid\")) #OUTPUT\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "source": [
    "### Optimization\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 0.5513 - accuracy: 0.7111 - val_loss: 0.3994 - val_accuracy: 0.8080\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 0s 903us/step - loss: 0.4134 - accuracy: 0.8029 - val_loss: 0.3332 - val_accuracy: 0.8455\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 0s 888us/step - loss: 0.3063 - accuracy: 0.8692 - val_loss: 0.2004 - val_accuracy: 0.9170\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 0s 846us/step - loss: 0.2187 - accuracy: 0.9102 - val_loss: 0.1622 - val_accuracy: 0.9270\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 0s 878us/step - loss: 0.1770 - accuracy: 0.9302 - val_loss: 0.1283 - val_accuracy: 0.9515\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 0s 878us/step - loss: 0.1372 - accuracy: 0.9482 - val_loss: 0.0965 - val_accuracy: 0.9620\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 0s 897us/step - loss: 0.1059 - accuracy: 0.9600 - val_loss: 0.1137 - val_accuracy: 0.9550\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 0s 891us/step - loss: 0.0893 - accuracy: 0.9672 - val_loss: 0.0502 - val_accuracy: 0.9830\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 0s 841us/step - loss: 0.0764 - accuracy: 0.9723 - val_loss: 0.0714 - val_accuracy: 0.9755\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 0s 856us/step - loss: 0.0612 - accuracy: 0.9791 - val_loss: 0.0571 - val_accuracy: 0.9760\n",
      "Epoch 11/30\n",
      "160/160 [==============================] - 0s 853us/step - loss: 0.0537 - accuracy: 0.9823 - val_loss: 0.0432 - val_accuracy: 0.9805\n",
      "Epoch 12/30\n",
      "160/160 [==============================] - 0s 910us/step - loss: 0.0518 - accuracy: 0.9836 - val_loss: 0.0127 - val_accuracy: 0.9965\n",
      "Epoch 13/30\n",
      "160/160 [==============================] - 0s 890us/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 0.0332 - val_accuracy: 0.9880\n",
      "Epoch 14/30\n",
      "160/160 [==============================] - 0s 852us/step - loss: 0.0357 - accuracy: 0.9867 - val_loss: 0.0108 - val_accuracy: 0.9970\n",
      "Epoch 15/30\n",
      "160/160 [==============================] - 0s 900us/step - loss: 0.0207 - accuracy: 0.9922 - val_loss: 0.0121 - val_accuracy: 0.9955\n",
      "Epoch 16/30\n",
      "160/160 [==============================] - 0s 719us/step - loss: 0.0254 - accuracy: 0.9926 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
      "Epoch 17/30\n",
      "160/160 [==============================] - 0s 716us/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0080 - val_accuracy: 0.9985\n",
      "Epoch 18/30\n",
      "160/160 [==============================] - 0s 728us/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0083 - val_accuracy: 0.9975\n",
      "Epoch 19/30\n",
      "160/160 [==============================] - 0s 728us/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9995\n",
      "Epoch 20/30\n",
      "160/160 [==============================] - 0s 721us/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.0038 - val_accuracy: 0.9975\n",
      "Epoch 21/30\n",
      "160/160 [==============================] - 0s 677us/step - loss: 0.0105 - accuracy: 0.9960 - val_loss: 0.0053 - val_accuracy: 0.9980\n",
      "Epoch 22/30\n",
      "160/160 [==============================] - 0s 682us/step - loss: 0.0109 - accuracy: 0.9963 - val_loss: 0.0198 - val_accuracy: 0.9920\n",
      "Epoch 23/30\n",
      "160/160 [==============================] - 0s 792us/step - loss: 0.0201 - accuracy: 0.9927 - val_loss: 0.0026 - val_accuracy: 0.9990\n",
      "Epoch 24/30\n",
      "160/160 [==============================] - 0s 734us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.6239e-04 - val_accuracy: 0.9995\n",
      "Epoch 25/30\n",
      "160/160 [==============================] - 0s 717us/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 3.7927e-05 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "160/160 [==============================] - 0s 721us/step - loss: 9.4296e-04 - accuracy: 0.9998 - val_loss: 1.4316e-05 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "160/160 [==============================] - 0s 734us/step - loss: 6.4040e-04 - accuracy: 1.0000 - val_loss: 7.3914e-06 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "160/160 [==============================] - 0s 720us/step - loss: 3.8132e-04 - accuracy: 1.0000 - val_loss: 4.2177e-06 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "160/160 [==============================] - 0s 709us/step - loss: 3.0688e-04 - accuracy: 1.0000 - val_loss: 2.8450e-06 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 3.1136e-04 - accuracy: 1.0000 - val_loss: 2.3041e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "opt1 = SGD(learning_rate=0.1,\n",
    "            momentum=0.9,\n",
    "            nesterov=True) \n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", #check what it is\n",
    "              optimizer=opt1, #\"adam\"\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "fit = model.fit(x_train, y_train, \n",
    "               epochs=30, batch_size=50,\n",
    "               validation_data=(x_test,y_test),\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}